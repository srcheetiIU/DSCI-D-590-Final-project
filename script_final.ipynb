{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Movie Script Generator"
      ],
      "metadata": {
        "id": "3ShSQA1o6-yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or7BuAVDpgyg",
        "outputId": "a92fa35b-35b7-42d1-bd76-29e54bed4801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_script(text):\n",
        "    \"\"\"\n",
        "    Preprocess the script by maintaining scene descriptions, dialogues, and special instructions.\n",
        "    Cleans and formats the text for consistency.\n",
        "    \"\"\"\n",
        "    # Split the script into lines\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Preprocessed script\n",
        "    preprocessed_script = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Remove any extraneous whitespace\n",
        "        line = line.strip()\n",
        "\n",
        "        # Skip empty lines\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Check for character dialogues (usually in all caps)\n",
        "        if line.isupper() and len(line.split()) <= 5:\n",
        "            # Add a marker for character names\n",
        "            line = f\"<CHARACTER>{line}</CHARACTER>\"\n",
        "\n",
        "        # Add the processed line to the preprocessed script\n",
        "        preprocessed_script.append(line)\n",
        "\n",
        "    return '\\n'.join(preprocessed_script)\n",
        "\n",
        "# Read the entire script\n",
        "file_path = '/content/inglorious_basterds_script.txt'  # Replace with your script file path\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    entire_script_content = file.read()\n",
        "\n",
        "# Preprocess the script\n",
        "preprocessed_script = preprocess_script(entire_script_content)\n",
        "\n",
        "# Save the preprocessed script to a new file\n",
        "preprocessed_file_path = '/content/preprocessed_script.txt'  # Replace with your desired file path\n",
        "with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(preprocessed_script)\n"
      ],
      "metadata": {
        "id": "x0fIrLtGphmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def tokenize_script(script_lines):\n",
        "    \"\"\"\n",
        "    Tokenizes the script by splitting each line into words, removing stop words and punctuation,\n",
        "    and creating a word-to-index mapping.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Remove punctuation and split each line into words\n",
        "    tokens = [word.strip(string.punctuation) for line in script_lines for word in line.split()]\n",
        "\n",
        "    # Filter out stop words and empty tokens\n",
        "    tokens = [word for word in tokens if word and word.lower() not in stop_words]\n",
        "\n",
        "    # Creating a counter of all words\n",
        "    word_counts = Counter(tokens)\n",
        "\n",
        "    # Sorting words according to their frequency\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    # Creating a word to index mapping (word -> integer)\n",
        "    word_to_index = {word: index for index, word in enumerate(sorted_vocab, 1)}  # starting index from 1\n",
        "\n",
        "    # Tokenizing the script\n",
        "    tokenized_script = [[word_to_index.get(word.strip(string.punctuation)) for word in line.split() if word.strip(string.punctuation) and word.lower() not in stop_words and word_to_index.get(word.strip(string.punctuation)) is not None] for line in script_lines]\n",
        "\n",
        "    return tokenized_script, word_to_index\n",
        "\n",
        "# Path to the preprocessed script file\n",
        "preprocessed_file_path = '/content/preprocessed_script.txt'  # Replace with your file path\n",
        "\n",
        "# Reading the preprocessed script\n",
        "with open(preprocessed_file_path, 'r', encoding='utf-8') as file:\n",
        "    script_content = file.read()\n",
        "\n",
        "# Splitting the script into a list of lines\n",
        "script_lines = script_content.split('\\n')\n",
        "\n",
        "# Tokenizing the script\n",
        "tokenized_script, word_to_index = tokenize_script(script_lines)\n",
        "\n",
        "# Displaying the first few tokenized lines and a snippet of the word_to_index mapping\n",
        "print(tokenized_script[:5])\n",
        "print({k: word_to_index[k] for k in list(word_to_index)[:10]})  # Showing first 10 words in the word_to_index dictionary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hc0cwpRpkQW",
        "outputId": "0f9ef6b8-9a21-4ac5-e697-17fd55eb2495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1112, 1113], [1114, 525, 611, 1524, 1525, 271, 16, 156, 1115, 401], [720, 464, 1526, 721], [402, 2373], [1527, 1528, 1529]]\n",
            "{'CHARACTER>COL': 1, 'LANDA</CHARACTER': 2, 'CHARACTER>LT': 3, 'German': 4, 'CHARACTER>SHOSANNA:</CHARACTER': 5, 'Shosanna': 6, 'ALDO</CHARACTER': 7, 'one': 8, 'Iâ€™m': 9, 'back': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assuming 'tokenized_script' and 'word_to_index' are available from your tokenization process\n",
        "\n",
        "# Prepare data for training\n",
        "def prepare_sequences(tokenized_script, sequence_length):\n",
        "    X, y = [], []\n",
        "    for line in tokenized_script:\n",
        "        for i in range(1, len(line)):\n",
        "            sequence = line[:i+1]\n",
        "            sequence = pad_sequences([sequence], maxlen=sequence_length, padding='pre')[0]\n",
        "            X.append(sequence[:-1])\n",
        "            y.append(sequence[-1])\n",
        "    return np.array(X), to_categorical(y, num_classes=len(word_to_index) + 1)\n"
      ],
      "metadata": {
        "id": "0EVshzjIpsk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sequence length and prepare sequences\n",
        "sequence_length = 50  # You can adjust this\n",
        "X, y = prepare_sequences(tokenized_script, sequence_length)"
      ],
      "metadata": {
        "id": "nwfs83jFpte9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kojD-esWoCdY"
      },
      "outputs": [],
      "source": [
        "def generate_text(seed_text, num_words, model, word_to_index, index_to_word):\n",
        "    text = []\n",
        "    for _ in range(num_words):\n",
        "        encoded = [word_to_index[word] for word in seed_text.split() if word in word_to_index]\n",
        "        encoded = pad_sequences([encoded], maxlen=sequence_length-1, padding='pre')\n",
        "        y_pred = model.predict(encoded, verbose=0)\n",
        "\n",
        "        # Get the index with the highest probability\n",
        "        predicted_index = np.argmax(y_pred, axis=-1)[0]\n",
        "        predicted_word = index_to_word[predicted_index]\n",
        "\n",
        "        seed_text += ' ' + predicted_word\n",
        "        text.append(predicted_word)\n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Your generate_text function here (make sure to define necessary variables like model, word_to_index, index_to_word, sequence_length)\n",
        "\n",
        "# Streamlit interface\n",
        "st.title('Movie Script Generation App')\n",
        "\n",
        "# Text input for seed_text\n",
        "seed_text = st.text_input('Seed Text', 'Enter your seed text here')\n",
        "\n",
        "# Numeric input for num_words\n",
        "num_words = st.number_input('Number of Words to Generate', min_value=1, max_value=100, value=25)\n",
        "\n",
        "model=tf.keras.models.load_model('/content/my_model.keras')\n",
        "# Button to generate text\n",
        "if st.button('Generate Text'):\n",
        "    generated_text = generate_text(seed_text, num_words, model, word_to_index, {v: k for k, v in word_to_index.items()})\n",
        "    st.text(\"Generated Text:\")\n",
        "    st.write(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "YY7nWr99oFG-",
        "outputId": "e741401d-1259-4190-809d-b115ef5a26ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4da97b211d4d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Your generate_text function here (make sure to define necessary variables like model, word_to_index, index_to_word, sequence_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}