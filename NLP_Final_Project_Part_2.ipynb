{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DSCI-D-590-Final-project\n",
        "##Movie Script Generator\n",
        "##Team- Sricharan Cheeti"
      ],
      "metadata": {
        "id": "UjATFxxth_hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "Q3bVMDM98xhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1.Splitting the Script: The script is initially split into individual lines, allowing each line to be processed separately.\n",
        "\n",
        "####2.Whitespace Removal: Extraneous whitespaces are removed from each line. This step helps in cleaning the script, ensuring that there are no unnecessary spaces at the beginning or end of each line.\n",
        "\n",
        "####3.Skipping Empty Lines: Empty lines are skipped. This is important for avoiding unnecessary gaps in the script, making it more compact and readable.\n",
        "\n",
        "####4.Identifying Character Dialogues: Lines that are in all uppercase and consist of five or fewer words are identified as character dialogues. This is based on the common scriptwriting convention where character names (preceding their dialogues) are often in all caps and short. You add a special marker <CHARACTER> around these lines to distinguish them as character dialogues. This can be particularly useful for later stages of script analysis or formatting, where distinguishing between dialogue and description is essential.\n",
        "\n",
        "####5.Reassembling the Script: Finally, the processed lines are reassembled back into a single text block, maintaining their original order."
      ],
      "metadata": {
        "id": "q9jg_JtCbqtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wPO0CzQ-1IT",
        "outputId": "1a6e1f06-fc60-4be5-cb68-005adc116346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElZf5Uhf8hMU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_script(text):\n",
        "    \"\"\"\n",
        "    Preprocess the script by maintaining scene descriptions, dialogues, and special instructions.\n",
        "    Cleans and formats the text for consistency.\n",
        "    \"\"\"\n",
        "    # Split the script into lines\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Preprocessed script\n",
        "    preprocessed_script = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Remove any extraneous whitespace\n",
        "        line = line.strip()\n",
        "\n",
        "        # Skip empty lines\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Check for character dialogues (usually in all caps)\n",
        "        if line.isupper() and len(line.split()) <= 5:\n",
        "            # Add a marker for character names\n",
        "            line = f\"<CHARACTER>{line}</CHARACTER>\"\n",
        "\n",
        "        # Add the processed line to the preprocessed script\n",
        "        preprocessed_script.append(line)\n",
        "\n",
        "    return '\\n'.join(preprocessed_script)\n",
        "\n",
        "# Read the entire script\n",
        "file_path = '/content/inglorious_basterds_script.txt'  # Replace with your script file path\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    entire_script_content = file.read()\n",
        "\n",
        "# Preprocess the script\n",
        "preprocessed_script = preprocess_script(entire_script_content)\n",
        "\n",
        "# Save the preprocessed script to a new file\n",
        "preprocessed_file_path = '/content/preprocessed_script.txt'  # Replace with your desired file path\n",
        "with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(preprocessed_script)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Extraction"
      ],
      "metadata": {
        "id": "q6MdCfaFhbc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tokenization:The script is broken down into individual words (tokens). This is done by splitting each line of the script into words.\n",
        "\n",
        "####Removing Stop Words and Punctuation:Stop words (common words that typically don't contribute to the meaning, like \"the\", \"is\", etc.) are removed. This is crucial because stop words can skew analyses by their frequency while contributing little to understanding the content.Punctuation is stripped from each word. This ensures that words are analyzed based on their textual content without being influenced by surrounding punctuation.\n",
        "\n",
        "###Word Frequency Count:A frequency count of all words (now free of stop words and punctuation) is performed using a Counter.This count helps in understanding the most common words in the script, which could be pivotal for thematic analysis.\n",
        "\n",
        "####Sorting and Indexing:The vocabulary is sorted according to word frequency, with the most frequent words first.A word-to-index mapping is created, where each unique word is assigned a unique integer. This is a standard practice in text analysis, facilitating various computational processes."
      ],
      "metadata": {
        "id": "iPVuaPDicdtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def tokenize_script(script_lines):\n",
        "    \"\"\"\n",
        "    Tokenizes the script by splitting each line into words, removing stop words and punctuation,\n",
        "    and creating a word-to-index mapping.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Remove punctuation and split each line into words\n",
        "    tokens = [word.strip(string.punctuation) for line in script_lines for word in line.split()]\n",
        "\n",
        "    # Filter out stop words and empty tokens\n",
        "    tokens = [word for word in tokens if word and word.lower() not in stop_words]\n",
        "\n",
        "    # Creating a counter of all words\n",
        "    word_counts = Counter(tokens)\n",
        "\n",
        "    # Sorting words according to their frequency\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    # Creating a word to index mapping (word -> integer)\n",
        "    word_to_index = {word: index for index, word in enumerate(sorted_vocab, 1)}  # starting index from 1\n",
        "\n",
        "    # Tokenizing the script\n",
        "    tokenized_script = [[word_to_index.get(word.strip(string.punctuation)) for word in line.split() if word.strip(string.punctuation) and word.lower() not in stop_words and word_to_index.get(word.strip(string.punctuation)) is not None] for line in script_lines]\n",
        "\n",
        "    return tokenized_script, word_to_index\n",
        "\n",
        "# Path to the preprocessed script file\n",
        "preprocessed_file_path = '/content/preprocessed_script.txt'  # Replace with your file path\n",
        "\n",
        "# Reading the preprocessed script\n",
        "with open(preprocessed_file_path, 'r', encoding='utf-8') as file:\n",
        "    script_content = file.read()\n",
        "\n",
        "# Splitting the script into a list of lines\n",
        "script_lines = script_content.split('\\n')\n",
        "\n",
        "# Tokenizing the script\n",
        "tokenized_script, word_to_index = tokenize_script(script_lines)\n",
        "\n",
        "# Displaying the first few tokenized lines and a snippet of the word_to_index mapping\n",
        "print(tokenized_script[:5])\n",
        "print({k: word_to_index[k] for k in list(word_to_index)[:10]})  # Showing first 10 words in the word_to_index dictionary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3ES48Jq8wxf",
        "outputId": "98d424e9-61be-4b88-a0ab-533e8f82481b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1112, 1113], [1114, 525, 611, 1524, 1525, 271, 16, 156, 1115, 401], [720, 464, 1526, 721], [402, 2373], [1527, 1528, 1529]]\n",
            "{'CHARACTER>COL': 1, 'LANDA</CHARACTER': 2, 'CHARACTER>LT': 3, 'German': 4, 'CHARACTER>SHOSANNA:</CHARACTER': 5, 'Shosanna': 6, 'ALDO</CHARACTER': 7, 'one': 8, 'I’m': 9, 'back': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main Functionality- Text Generation"
      ],
      "metadata": {
        "id": "qyFivrkthrwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Model Architecture:\n",
        "\n",
        "####Embedding Layer: The model begins with an embedding layer, which transforms each word-index in the input sequences into a dense vector of fixed size. This is a crucial step as it allows the model to learn a rich representation for each word.\n",
        "####LSTM Layers: Next are two LSTM (Long Short-Term Memory) layers. LSTM, a form of recurrent neural network (RNN), is adept at processing sequences and capturing temporal dependencies. This is essential for text generation, where the meaning and structure depend heavily on the order of words.\n",
        "####Dropout Layer: A dropout layer follows, which helps prevent overfitting by randomly setting a portion of the input units to zero during training.\n",
        "####Dense Layer with Softmax Activation: The final layer is a dense (fully connected) layer with softmax activation. It outputs a probability distribution over the entire vocabulary for the next word in the sequence.\n",
        "####2. Training Process:\n",
        "####Data Preparation: The training data is prepared by creating sequences from the tokenized script. Each input sequence is padded to a fixed length and used to predict the next word in the sequence.\n",
        "####One-Hot Encoding: The output words (labels) are one-hot encoded, turning them into a format suitable for categorical prediction.\n",
        "####Model Compilation: The model is compiled with categorical crossentropy as the loss function and the Adam optimizer, a combination well-suited for classification tasks.\n",
        "####3. Text Generation:\n",
        "####Generating New Text: Once trained, the model can generate new text. Starting with a seed sequence, the model predicts the next word, which is then appended to the sequence. This new sequence is fed back into the model for the next prediction. This process repeats, generating a chain of text.\n",
        "####Handling Sequences: The model continuously updates the input sequence by adding new predictions and trimming to maintain the fixed length.\n",
        "####Probabilistic Nature: Each output from the model represents a probability distribution over the possible next words, allowing for diverse and contextually relevant text generation.\n",
        "####4. Application:\n",
        "####This functionality is particularly exciting for creative applications like:\n",
        "\n",
        "####Automated Script Writing: Generating new script content that aligns with the style and themes of the input script.\n",
        "####Predictive Text Generation: Offering suggestions for scriptwriters based on the current context of their writing.\n",
        "####5. Significance:\n",
        "####The significance of this model lies in its ability to learn and mimic the linguistic patterns, style, and narrative flow of the input script, providing a tool that can assist in creative writing or even generate entirely new content based on learned patterns. It represents a blend of machine learning and creative writing, pushing the boundaries of how AI can be used in artistic and creative domains.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cqEnaxBNdn08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assuming 'tokenized_script' and 'word_to_index' are available from your tokenization process\n",
        "\n",
        "# Prepare data for training\n",
        "def prepare_sequences(tokenized_script, sequence_length):\n",
        "    X, y = [], []\n",
        "    for line in tokenized_script:\n",
        "        for i in range(1, len(line)):\n",
        "            sequence = line[:i+1]\n",
        "            sequence = pad_sequences([sequence], maxlen=sequence_length, padding='pre')[0]\n",
        "            X.append(sequence[:-1])\n",
        "            y.append(sequence[-1])\n",
        "    return np.array(X), to_categorical(y, num_classes=len(word_to_index) + 1)\n"
      ],
      "metadata": {
        "id": "dsGW6jiu_GpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sequence length and prepare sequences\n",
        "sequence_length = 50  # You can adjust this\n",
        "X, y = prepare_sequences(tokenized_script, sequence_length)"
      ],
      "metadata": {
        "id": "Jz7XXg_SBYJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_to_index) + 1, output_dim=100, input_length=sequence_length-1))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(word_to_index) + 1, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "rwibGvrXBb4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSkwdSQzBfdy",
        "outputId": "9f545bca-ee1e-43a1-e750-bb3008918627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "116/116 [==============================] - 41s 348ms/step - loss: 6.9716\n",
            "Epoch 2/100\n",
            "116/116 [==============================] - 41s 355ms/step - loss: 6.9082\n",
            "Epoch 3/100\n",
            "116/116 [==============================] - 41s 356ms/step - loss: 6.8491\n",
            "Epoch 4/100\n",
            "116/116 [==============================] - 41s 357ms/step - loss: 6.7897\n",
            "Epoch 5/100\n",
            "116/116 [==============================] - 41s 355ms/step - loss: 6.7302\n",
            "Epoch 6/100\n",
            "116/116 [==============================] - 41s 353ms/step - loss: 6.6731\n",
            "Epoch 7/100\n",
            "116/116 [==============================] - 40s 346ms/step - loss: 6.6181\n",
            "Epoch 8/100\n",
            "116/116 [==============================] - 41s 354ms/step - loss: 6.5597\n",
            "Epoch 9/100\n",
            "116/116 [==============================] - 41s 358ms/step - loss: 6.5057\n",
            "Epoch 10/100\n",
            "116/116 [==============================] - 41s 355ms/step - loss: 6.4512\n",
            "Epoch 11/100\n",
            "116/116 [==============================] - 41s 357ms/step - loss: 6.3968\n",
            "Epoch 12/100\n",
            "116/116 [==============================] - 42s 359ms/step - loss: 6.3451\n",
            "Epoch 13/100\n",
            "116/116 [==============================] - 42s 362ms/step - loss: 6.2945\n",
            "Epoch 14/100\n",
            "116/116 [==============================] - 42s 359ms/step - loss: 6.2454\n",
            "Epoch 15/100\n",
            "116/116 [==============================] - 41s 350ms/step - loss: 6.1994\n",
            "Epoch 16/100\n",
            "116/116 [==============================] - 41s 348ms/step - loss: 6.1507\n",
            "Epoch 17/100\n",
            "116/116 [==============================] - 42s 357ms/step - loss: 6.1038\n",
            "Epoch 18/100\n",
            "116/116 [==============================] - 41s 357ms/step - loss: 6.0586\n",
            "Epoch 19/100\n",
            "116/116 [==============================] - 41s 355ms/step - loss: 6.0122\n",
            "Epoch 20/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 5.9702\n",
            "Epoch 21/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 5.9264\n",
            "Epoch 22/100\n",
            "116/116 [==============================] - 42s 359ms/step - loss: 5.8865\n",
            "Epoch 23/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 5.8428\n",
            "Epoch 24/100\n",
            "116/116 [==============================] - 42s 362ms/step - loss: 5.8036\n",
            "Epoch 25/100\n",
            "116/116 [==============================] - 41s 353ms/step - loss: 5.7623\n",
            "Epoch 26/100\n",
            "116/116 [==============================] - 41s 351ms/step - loss: 5.7223\n",
            "Epoch 27/100\n",
            "116/116 [==============================] - 42s 359ms/step - loss: 5.6811\n",
            "Epoch 28/100\n",
            "116/116 [==============================] - 42s 358ms/step - loss: 5.6435\n",
            "Epoch 29/100\n",
            "116/116 [==============================] - 41s 357ms/step - loss: 5.6052\n",
            "Epoch 30/100\n",
            "116/116 [==============================] - 42s 359ms/step - loss: 5.5686\n",
            "Epoch 31/100\n",
            "116/116 [==============================] - 42s 364ms/step - loss: 5.5321\n",
            "Epoch 32/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 5.4966\n",
            "Epoch 33/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 5.4588\n",
            "Epoch 34/100\n",
            "116/116 [==============================] - 42s 362ms/step - loss: 5.4232\n",
            "Epoch 35/100\n",
            "116/116 [==============================] - 41s 351ms/step - loss: 5.3880\n",
            "Epoch 36/100\n",
            "116/116 [==============================] - 41s 354ms/step - loss: 5.3528\n",
            "Epoch 37/100\n",
            "116/116 [==============================] - 41s 356ms/step - loss: 5.3172\n",
            "Epoch 38/100\n",
            "116/116 [==============================] - 41s 358ms/step - loss: 5.2774\n",
            "Epoch 39/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 5.2446\n",
            "Epoch 40/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 5.2082\n",
            "Epoch 41/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 5.1731\n",
            "Epoch 42/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 5.1372\n",
            "Epoch 43/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 5.1079\n",
            "Epoch 44/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 5.0696\n",
            "Epoch 45/100\n",
            "116/116 [==============================] - 41s 356ms/step - loss: 5.0440\n",
            "Epoch 46/100\n",
            "116/116 [==============================] - 41s 350ms/step - loss: 5.0105\n",
            "Epoch 47/100\n",
            "116/116 [==============================] - 42s 357ms/step - loss: 4.9791\n",
            "Epoch 48/100\n",
            "116/116 [==============================] - 42s 363ms/step - loss: 4.9454\n",
            "Epoch 49/100\n",
            "116/116 [==============================] - 42s 359ms/step - loss: 4.9204\n",
            "Epoch 50/100\n",
            "116/116 [==============================] - 42s 364ms/step - loss: 4.8832\n",
            "Epoch 51/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 4.8521\n",
            "Epoch 52/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 4.8220\n",
            "Epoch 53/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 4.7953\n",
            "Epoch 54/100\n",
            "116/116 [==============================] - 42s 362ms/step - loss: 4.7647\n",
            "Epoch 55/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 4.7376\n",
            "Epoch 56/100\n",
            "116/116 [==============================] - 41s 356ms/step - loss: 4.7022\n",
            "Epoch 57/100\n",
            "116/116 [==============================] - 40s 347ms/step - loss: 4.6764\n",
            "Epoch 58/100\n",
            "116/116 [==============================] - 42s 358ms/step - loss: 4.6455\n",
            "Epoch 59/100\n",
            "116/116 [==============================] - 42s 362ms/step - loss: 4.6148\n",
            "Epoch 60/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 4.5875\n",
            "Epoch 61/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 4.5572\n",
            "Epoch 62/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 4.5295\n",
            "Epoch 63/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 4.5008\n",
            "Epoch 64/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 4.4730\n",
            "Epoch 65/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 4.4420\n",
            "Epoch 66/100\n",
            "116/116 [==============================] - 42s 363ms/step - loss: 4.4125\n",
            "Epoch 67/100\n",
            "116/116 [==============================] - 42s 363ms/step - loss: 4.3834\n",
            "Epoch 68/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 4.3541\n",
            "Epoch 69/100\n",
            "116/116 [==============================] - 42s 356ms/step - loss: 4.3256\n",
            "Epoch 70/100\n",
            "116/116 [==============================] - 41s 355ms/step - loss: 4.2955\n",
            "Epoch 71/100\n",
            "116/116 [==============================] - 42s 360ms/step - loss: 4.2694\n",
            "Epoch 72/100\n",
            "116/116 [==============================] - 42s 359ms/step - loss: 4.2367\n",
            "Epoch 73/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 4.2112\n",
            "Epoch 74/100\n",
            "116/116 [==============================] - 42s 363ms/step - loss: 4.1803\n",
            "Epoch 75/100\n",
            "116/116 [==============================] - 42s 362ms/step - loss: 4.1554\n",
            "Epoch 76/100\n",
            "116/116 [==============================] - 42s 364ms/step - loss: 4.1276\n",
            "Epoch 77/100\n",
            "116/116 [==============================] - 42s 362ms/step - loss: 4.0958\n",
            "Epoch 78/100\n",
            "116/116 [==============================] - 42s 364ms/step - loss: 4.0710\n",
            "Epoch 79/100\n",
            "116/116 [==============================] - 42s 358ms/step - loss: 4.0413\n",
            "Epoch 80/100\n",
            "116/116 [==============================] - 41s 354ms/step - loss: 4.0120\n",
            "Epoch 81/100\n",
            "116/116 [==============================] - 41s 355ms/step - loss: 3.9874\n",
            "Epoch 82/100\n",
            "116/116 [==============================] - 42s 361ms/step - loss: 3.9537\n",
            "Epoch 83/100\n",
            "116/116 [==============================] - 42s 365ms/step - loss: 3.9204\n",
            "Epoch 84/100\n",
            "116/116 [==============================] - 42s 366ms/step - loss: 3.8968\n",
            "Epoch 85/100\n",
            "116/116 [==============================] - 42s 364ms/step - loss: 3.8695\n",
            "Epoch 86/100\n",
            "116/116 [==============================] - 42s 363ms/step - loss: 3.8380\n",
            "Epoch 87/100\n",
            "116/116 [==============================] - 42s 366ms/step - loss: 3.8140\n",
            "Epoch 88/100\n",
            "116/116 [==============================] - 42s 365ms/step - loss: 3.7856\n",
            "Epoch 89/100\n",
            "116/116 [==============================] - 42s 364ms/step - loss: 3.7567\n",
            "Epoch 90/100\n",
            "116/116 [==============================] - 42s 363ms/step - loss: 3.7311\n",
            "Epoch 91/100\n",
            "116/116 [==============================] - 42s 364ms/step - loss: 3.7061\n",
            "Epoch 92/100\n",
            "116/116 [==============================] - 42s 363ms/step - loss: 3.6782\n",
            "Epoch 93/100\n",
            "116/116 [==============================] - 42s 362ms/step - loss: 3.6528\n",
            "Epoch 94/100\n",
            "116/116 [==============================] - 41s 356ms/step - loss: 3.6307\n",
            "Epoch 95/100\n",
            "116/116 [==============================] - 41s 353ms/step - loss: 3.6033\n",
            "Epoch 96/100\n",
            "116/116 [==============================] - 42s 358ms/step - loss: 3.5703\n",
            "Epoch 97/100\n",
            "116/116 [==============================] - 43s 371ms/step - loss: 3.5416\n",
            "Epoch 98/100\n",
            "116/116 [==============================] - 43s 368ms/step - loss: 3.5194\n",
            "Epoch 99/100\n",
            "116/116 [==============================] - 42s 365ms/step - loss: 3.4943\n",
            "Epoch 100/100\n",
            "116/116 [==============================] - 42s 365ms/step - loss: 3.4668\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x798fbaf7d0c0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, num_words, model, word_to_index, index_to_word):\n",
        "    text = []\n",
        "    for _ in range(num_words):\n",
        "        encoded = [word_to_index[word] for word in seed_text.split() if word in word_to_index]\n",
        "        encoded = pad_sequences([encoded], maxlen=sequence_length-1, padding='pre')\n",
        "        y_pred = model.predict(encoded, verbose=0)\n",
        "\n",
        "        # Get the index with the highest probability\n",
        "        predicted_index = np.argmax(y_pred, axis=-1)[0]\n",
        "        predicted_word = index_to_word[predicted_index]\n",
        "\n",
        "        seed_text += ' ' + predicted_word\n",
        "        text.append(predicted_word)\n",
        "    return ' '.join(text)\n"
      ],
      "metadata": {
        "id": "SybP6GG8DQKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"The French farmer\"\n",
        "generated_text = generate_text(seed_text, 500, model, word_to_index, {v: k for k, v in word_to_index.items()})\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mikvQ75rDUqY",
        "outputId": "f3b4fe54-080d-45f9-90be-21153d142996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sinks convoy en Cramming noodle back pants weight door killin’ sheer violence Marcel sat sympathetic French beam emanating neck violence lion HAMMERSMARK puffing underneath man soldier entire black feet gold tableau certain Palu NAZI ENLISTED MEN moved napkin wall gonna one truck opposite end table Nazi knife marquee saying behind French dialogue going Basterds worse German Herrman dead Nazi caricatures ladies burning precede makes grab KNIFE rodent skin Monsieur LaPadite rumors heard regarding back us agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German classic films However anyway winning beyond lie hostess medical pop file cinema approaching gold Nancy pipe Shosanna’s cinema cry like week guards/ushers now-vacant cinema Goering proper neighbor They’ve marquee SPRAYS appears left resembles place indication war tonight ’bout table It’s agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German classic films However anyway winning beyond lie hostess medical pop file cinema approaching gold Nancy pipe Shosanna’s cinema cry like week guards/ushers now-vacant cinema Goering proper neighbor They’ve marquee SPRAYS appears left resembles place indication war tonight ’bout table It’s agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German classic films However anyway winning beyond lie hostess medical pop file cinema approaching gold Nancy pipe Shosanna’s cinema cry like week guards/ushers now-vacant cinema Goering proper neighbor They’ve marquee SPRAYS appears left resembles place indication war tonight ’bout table It’s agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German classic films However anyway winning beyond lie hostess medical pop file cinema approaching gold Nancy pipe Shosanna’s cinema cry like week guards/ushers now-vacant cinema Goering proper neighbor They’ve marquee SPRAYS appears left resembles place indication war tonight ’bout table It’s agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Personal Contribution Statement"
      ],
      "metadata": {
        "id": "ZF7Fz-TzfNPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####As I am working on a Solo project, all the tasks were contributed by me.The development of the text generation model from a movie script was intensive and strategically planned over a three-week period, encompassing various key stages of the project.\n",
        "\n",
        "####Week 1: Script Preparation and Feature Extraction\n",
        "\n",
        "Script Preprocessing: Initiated the project by cleaning, formatting, and structurally organizing the movie script for computational analysis.\n",
        "Feature Extraction: Implemented tokenization of the script, removing stop words and punctuation, and created a structured dataset. This included building a frequency-based vocabulary essential for the model training.\n",
        "####Week 2: Model Development and Initial Training\n",
        "\n",
        "Data Preparation: Prepared the data for the neural network by generating and formatting sequences from the tokenized script, including one-hot encoding of output labels.\n",
        "Model Design and Commencement of Training: Designed the LSTM-based neural network model, selecting the appropriate architecture and configuring layers. Initiated the model training, focusing on embedding, LSTM layers, dropout, and dense layers.\n",
        "####Week 3: Model Optimization and Text Generation\n",
        "\n",
        "Model Fine-Tuning: Continued with the training of the model, making iterative adjustments and refinements to optimize its performance and accuracy.\n",
        "Text Generation Implementation and Testing: Developed the text generation functionality to enable the model to produce new script content. Conducted thorough testing and evaluation of the generated text, ensuring coherence and alignment with the style of the input script."
      ],
      "metadata": {
        "id": "eBqoDkLnfUe-"
      }
    }
  ]
}