{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DSCI-D-590-Final-project\n",
        "##Movie Script Generator\n",
        "##Team- Sricharan Cheeti"
      ],
      "metadata": {
        "id": "UjATFxxth_hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "Q3bVMDM98xhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1.Splitting the Script: The script is initially split into individual lines, allowing each line to be processed separately.\n",
        "\n",
        "####2.Whitespace Removal: Extraneous whitespaces are removed from each line. This step helps in cleaning the script, ensuring that there are no unnecessary spaces at the beginning or end of each line.\n",
        "\n",
        "####3.Skipping Empty Lines: Empty lines are skipped. This is important for avoiding unnecessary gaps in the script, making it more compact and readable.\n",
        "\n",
        "####4.Identifying Character Dialogues: Lines that are in all uppercase and consist of five or fewer words are identified as character dialogues. This is based on the common scriptwriting convention where character names (preceding their dialogues) are often in all caps and short. You add a special marker <CHARACTER> around these lines to distinguish them as character dialogues. This can be particularly useful for later stages of script analysis or formatting, where distinguishing between dialogue and description is essential.\n",
        "\n",
        "####5.Reassembling the Script: Finally, the processed lines are reassembled back into a single text block, maintaining their original order."
      ],
      "metadata": {
        "id": "q9jg_JtCbqtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wPO0CzQ-1IT",
        "outputId": "d5f2ffe3-dcb6-4242-b631-40c52e6e162e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElZf5Uhf8hMU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_script(text):\n",
        "    \"\"\"\n",
        "    Preprocess the script by maintaining scene descriptions, dialogues, and special instructions.\n",
        "    Cleans and formats the text for consistency.\n",
        "    \"\"\"\n",
        "    # Split the script into lines\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Preprocessed script\n",
        "    preprocessed_script = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Remove any extraneous whitespace\n",
        "        line = line.strip()\n",
        "\n",
        "        # Skip empty lines\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Check for character dialogues (usually in all caps)\n",
        "        if line.isupper() and len(line.split()) <= 5:\n",
        "            # Add a marker for character names\n",
        "            line = f\"<CHARACTER>{line}</CHARACTER>\"\n",
        "\n",
        "        # Add the processed line to the preprocessed script\n",
        "        preprocessed_script.append(line)\n",
        "\n",
        "    return '\\n'.join(preprocessed_script)\n",
        "\n",
        "# Read the entire script\n",
        "file_path = '/content/inglorious_basterds_script.txt'  # Replace with your script file path\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    entire_script_content = file.read()\n",
        "\n",
        "# Preprocess the script\n",
        "preprocessed_script = preprocess_script(entire_script_content)\n",
        "\n",
        "# Save the preprocessed script to a new file\n",
        "preprocessed_file_path = '/content/preprocessed_script.txt'  # Replace with your desired file path\n",
        "with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(preprocessed_script)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Extraction"
      ],
      "metadata": {
        "id": "q6MdCfaFhbc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tokenization:The script is broken down into individual words (tokens). This is done by splitting each line of the script into words.\n",
        "\n",
        "####Removing Stop Words and Punctuation:Stop words (common words that typically don't contribute to the meaning, like \"the\", \"is\", etc.) are removed. This is crucial because stop words can skew analyses by their frequency while contributing little to understanding the content.Punctuation is stripped from each word. This ensures that words are analyzed based on their textual content without being influenced by surrounding punctuation.\n",
        "\n",
        "###Word Frequency Count:A frequency count of all words (now free of stop words and punctuation) is performed using a Counter.This count helps in understanding the most common words in the script, which could be pivotal for thematic analysis.\n",
        "\n",
        "####Sorting and Indexing:The vocabulary is sorted according to word frequency, with the most frequent words first.A word-to-index mapping is created, where each unique word is assigned a unique integer. This is a standard practice in text analysis, facilitating various computational processes."
      ],
      "metadata": {
        "id": "iPVuaPDicdtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def tokenize_script(script_lines):\n",
        "    \"\"\"\n",
        "    Tokenizes the script by splitting each line into words, removing stop words and punctuation,\n",
        "    and creating a word-to-index mapping.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Remove punctuation and split each line into words\n",
        "    tokens = [word.strip(string.punctuation) for line in script_lines for word in line.split()]\n",
        "\n",
        "    # Filter out stop words and empty tokens\n",
        "    tokens = [word for word in tokens if word and word.lower() not in stop_words]\n",
        "\n",
        "    # Creating a counter of all words\n",
        "    word_counts = Counter(tokens)\n",
        "\n",
        "    # Sorting words according to their frequency\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    # Creating a word to index mapping (word -> integer)\n",
        "    word_to_index = {word: index for index, word in enumerate(sorted_vocab, 1)}  # starting index from 1\n",
        "\n",
        "    # Tokenizing the script\n",
        "    tokenized_script = [[word_to_index.get(word.strip(string.punctuation)) for word in line.split() if word.strip(string.punctuation) and word.lower() not in stop_words and word_to_index.get(word.strip(string.punctuation)) is not None] for line in script_lines]\n",
        "\n",
        "    return tokenized_script, word_to_index\n",
        "\n",
        "# Path to the preprocessed script file\n",
        "preprocessed_file_path = '/content/preprocessed_script.txt'  # Replace with your file path\n",
        "\n",
        "# Reading the preprocessed script\n",
        "with open(preprocessed_file_path, 'r', encoding='utf-8') as file:\n",
        "    script_content = file.read()\n",
        "\n",
        "# Splitting the script into a list of lines\n",
        "script_lines = script_content.split('\\n')\n",
        "\n",
        "# Tokenizing the script\n",
        "tokenized_script, word_to_index = tokenize_script(script_lines)\n",
        "\n",
        "# Displaying the first few tokenized lines and a snippet of the word_to_index mapping\n",
        "print(tokenized_script[:5])\n",
        "print({k: word_to_index[k] for k in list(word_to_index)[:10]})  # Showing first 10 words in the word_to_index dictionary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3ES48Jq8wxf",
        "outputId": "08d9eec3-5741-4af3-9499-10976e06f639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1112, 1113], [1114, 525, 611, 1524, 1525, 271, 16, 156, 1115, 401], [720, 464, 1526, 721], [402, 2373], [1527, 1528, 1529]]\n",
            "{'CHARACTER>COL': 1, 'LANDA</CHARACTER': 2, 'CHARACTER>LT': 3, 'German': 4, 'CHARACTER>SHOSANNA:</CHARACTER': 5, 'Shosanna': 6, 'ALDO</CHARACTER': 7, 'one': 8, 'I’m': 9, 'back': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main Functionality- Text Generation"
      ],
      "metadata": {
        "id": "qyFivrkthrwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Model Architecture:\n",
        "\n",
        "####Embedding Layer: The model begins with an embedding layer, which transforms each word-index in the input sequences into a dense vector of fixed size. This is a crucial step as it allows the model to learn a rich representation for each word.\n",
        "####LSTM Layers: Next are two LSTM (Long Short-Term Memory) layers. LSTM, a form of recurrent neural network (RNN), is adept at processing sequences and capturing temporal dependencies. This is essential for text generation, where the meaning and structure depend heavily on the order of words.\n",
        "####Dropout Layer: A dropout layer follows, which helps prevent overfitting by randomly setting a portion of the input units to zero during training.\n",
        "####Dense Layer with Softmax Activation: The final layer is a dense (fully connected) layer with softmax activation. It outputs a probability distribution over the entire vocabulary for the next word in the sequence.\n",
        "####2. Training Process:\n",
        "####Data Preparation: The training data is prepared by creating sequences from the tokenized script. Each input sequence is padded to a fixed length and used to predict the next word in the sequence.\n",
        "####One-Hot Encoding: The output words (labels) are one-hot encoded, turning them into a format suitable for categorical prediction.\n",
        "####Model Compilation: The model is compiled with categorical crossentropy as the loss function and the Adam optimizer, a combination well-suited for classification tasks.\n",
        "####3. Text Generation:\n",
        "####Generating New Text: Once trained, the model can generate new text. Starting with a seed sequence, the model predicts the next word, which is then appended to the sequence. This new sequence is fed back into the model for the next prediction. This process repeats, generating a chain of text.\n",
        "####Handling Sequences: The model continuously updates the input sequence by adding new predictions and trimming to maintain the fixed length.\n",
        "####Probabilistic Nature: Each output from the model represents a probability distribution over the possible next words, allowing for diverse and contextually relevant text generation.\n",
        "####4. Application:\n",
        "####This functionality is particularly exciting for creative applications like:\n",
        "\n",
        "####Automated Script Writing: Generating new script content that aligns with the style and themes of the input script.\n",
        "####Predictive Text Generation: Offering suggestions for scriptwriters based on the current context of their writing.\n",
        "####5. Significance:\n",
        "####The significance of this model lies in its ability to learn and mimic the linguistic patterns, style, and narrative flow of the input script, providing a tool that can assist in creative writing or even generate entirely new content based on learned patterns. It represents a blend of machine learning and creative writing, pushing the boundaries of how AI can be used in artistic and creative domains.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cqEnaxBNdn08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assuming 'tokenized_script' and 'word_to_index' are available from your tokenization process\n",
        "\n",
        "# Prepare data for training\n",
        "def prepare_sequences(tokenized_script, sequence_length):\n",
        "    X, y = [], []\n",
        "    for line in tokenized_script:\n",
        "        for i in range(1, len(line)):\n",
        "            sequence = line[:i+1]\n",
        "            sequence = pad_sequences([sequence], maxlen=sequence_length, padding='pre')[0]\n",
        "            X.append(sequence[:-1])\n",
        "            y.append(sequence[-1])\n",
        "    return np.array(X), to_categorical(y, num_classes=len(word_to_index) + 1)\n"
      ],
      "metadata": {
        "id": "dsGW6jiu_GpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sequence length and prepare sequences\n",
        "sequence_length = 50  # You can adjust this\n",
        "X, y = prepare_sequences(tokenized_script, sequence_length)"
      ],
      "metadata": {
        "id": "Jz7XXg_SBYJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_to_index) + 1, output_dim=100, input_length=sequence_length-1))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(word_to_index) + 1, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "rwibGvrXBb4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSkwdSQzBfdy",
        "outputId": "2b1463de-5d5b-4bf2-d6f8-a4955672f1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "116/116 [==============================] - 55s 426ms/step - loss: 8.2077\n",
            "Epoch 2/100\n",
            "116/116 [==============================] - 48s 413ms/step - loss: 7.7819\n",
            "Epoch 3/100\n",
            "116/116 [==============================] - 48s 415ms/step - loss: 7.6138\n",
            "Epoch 4/100\n",
            "116/116 [==============================] - 48s 410ms/step - loss: 7.4992\n",
            "Epoch 5/100\n",
            "116/116 [==============================] - 48s 416ms/step - loss: 7.4181\n",
            "Epoch 6/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 7.3498\n",
            "Epoch 7/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 7.2740\n",
            "Epoch 8/100\n",
            "116/116 [==============================] - 47s 404ms/step - loss: 7.1942\n",
            "Epoch 9/100\n",
            "116/116 [==============================] - 49s 423ms/step - loss: 7.1161\n",
            "Epoch 10/100\n",
            "116/116 [==============================] - 48s 412ms/step - loss: 7.0350\n",
            "Epoch 11/100\n",
            "116/116 [==============================] - 47s 403ms/step - loss: 6.9595\n",
            "Epoch 12/100\n",
            "116/116 [==============================] - 47s 404ms/step - loss: 6.8898\n",
            "Epoch 13/100\n",
            "116/116 [==============================] - 47s 406ms/step - loss: 6.8216\n",
            "Epoch 14/100\n",
            "116/116 [==============================] - 48s 416ms/step - loss: 6.7592\n",
            "Epoch 15/100\n",
            "116/116 [==============================] - 50s 431ms/step - loss: 6.6965\n",
            "Epoch 16/100\n",
            "116/116 [==============================] - 47s 407ms/step - loss: 6.6364\n",
            "Epoch 17/100\n",
            "116/116 [==============================] - 47s 404ms/step - loss: 6.5813\n",
            "Epoch 18/100\n",
            "116/116 [==============================] - 48s 416ms/step - loss: 6.5239\n",
            "Epoch 19/100\n",
            "116/116 [==============================] - 48s 408ms/step - loss: 6.4728\n",
            "Epoch 20/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 6.4184\n",
            "Epoch 21/100\n",
            "116/116 [==============================] - 47s 405ms/step - loss: 6.3717\n",
            "Epoch 22/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 6.3154\n",
            "Epoch 23/100\n",
            "116/116 [==============================] - 49s 420ms/step - loss: 6.2652\n",
            "Epoch 24/100\n",
            "116/116 [==============================] - 48s 411ms/step - loss: 6.2137\n",
            "Epoch 25/100\n",
            "116/116 [==============================] - 47s 406ms/step - loss: 6.1587\n",
            "Epoch 26/100\n",
            "116/116 [==============================] - 47s 409ms/step - loss: 6.1056\n",
            "Epoch 27/100\n",
            "116/116 [==============================] - 48s 414ms/step - loss: 6.0548\n",
            "Epoch 28/100\n",
            "116/116 [==============================] - 48s 416ms/step - loss: 6.0090\n",
            "Epoch 29/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 5.9560\n",
            "Epoch 30/100\n",
            "116/116 [==============================] - 47s 409ms/step - loss: 5.9093\n",
            "Epoch 31/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 5.8597\n",
            "Epoch 32/100\n",
            "116/116 [==============================] - 49s 424ms/step - loss: 5.8114\n",
            "Epoch 33/100\n",
            "116/116 [==============================] - 47s 405ms/step - loss: 5.7680\n",
            "Epoch 34/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 5.7213\n",
            "Epoch 35/100\n",
            "116/116 [==============================] - 47s 410ms/step - loss: 5.6744\n",
            "Epoch 36/100\n",
            "116/116 [==============================] - 49s 419ms/step - loss: 5.6283\n",
            "Epoch 37/100\n",
            "116/116 [==============================] - 48s 413ms/step - loss: 5.5816\n",
            "Epoch 38/100\n",
            "116/116 [==============================] - 47s 407ms/step - loss: 5.5395\n",
            "Epoch 39/100\n",
            "116/116 [==============================] - 47s 407ms/step - loss: 5.4991\n",
            "Epoch 40/100\n",
            "116/116 [==============================] - 47s 409ms/step - loss: 5.4520\n",
            "Epoch 41/100\n",
            "116/116 [==============================] - 49s 424ms/step - loss: 5.4126\n",
            "Epoch 42/100\n",
            "116/116 [==============================] - 47s 409ms/step - loss: 5.3694\n",
            "Epoch 43/100\n",
            "116/116 [==============================] - 47s 409ms/step - loss: 5.3238\n",
            "Epoch 44/100\n",
            "116/116 [==============================] - 47s 405ms/step - loss: 5.2868\n",
            "Epoch 45/100\n",
            "116/116 [==============================] - 48s 415ms/step - loss: 5.2429\n",
            "Epoch 46/100\n",
            "116/116 [==============================] - 48s 411ms/step - loss: 5.1981\n",
            "Epoch 47/100\n",
            "116/116 [==============================] - 47s 405ms/step - loss: 5.1615\n",
            "Epoch 48/100\n",
            "116/116 [==============================] - 47s 405ms/step - loss: 5.1209\n",
            "Epoch 49/100\n",
            "116/116 [==============================] - 47s 406ms/step - loss: 5.0782\n",
            "Epoch 50/100\n",
            "116/116 [==============================] - 49s 419ms/step - loss: 5.0368\n",
            "Epoch 51/100\n",
            "116/116 [==============================] - 47s 405ms/step - loss: 4.9980\n",
            "Epoch 52/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 4.9621\n",
            "Epoch 53/100\n",
            "116/116 [==============================] - 47s 406ms/step - loss: 4.9224\n",
            "Epoch 54/100\n",
            "116/116 [==============================] - 47s 403ms/step - loss: 4.8771\n",
            "Epoch 55/100\n",
            "116/116 [==============================] - 48s 415ms/step - loss: 4.8461\n",
            "Epoch 56/100\n",
            "116/116 [==============================] - 48s 410ms/step - loss: 4.7984\n",
            "Epoch 57/100\n",
            "116/116 [==============================] - 48s 415ms/step - loss: 4.7574\n",
            "Epoch 58/100\n",
            "116/116 [==============================] - 47s 405ms/step - loss: 4.7136\n",
            "Epoch 59/100\n",
            "116/116 [==============================] - 48s 415ms/step - loss: 4.6797\n",
            "Epoch 60/100\n",
            "116/116 [==============================] - 48s 414ms/step - loss: 4.6361\n",
            "Epoch 61/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 4.5937\n",
            "Epoch 62/100\n",
            "116/116 [==============================] - 47s 407ms/step - loss: 4.5546\n",
            "Epoch 63/100\n",
            "116/116 [==============================] - 47s 406ms/step - loss: 4.5124\n",
            "Epoch 64/100\n",
            "116/116 [==============================] - 49s 423ms/step - loss: 4.4726\n",
            "Epoch 65/100\n",
            "116/116 [==============================] - 48s 409ms/step - loss: 4.4313\n",
            "Epoch 66/100\n",
            "116/116 [==============================] - 48s 414ms/step - loss: 4.3854\n",
            "Epoch 67/100\n",
            "116/116 [==============================] - 48s 413ms/step - loss: 4.3532\n",
            "Epoch 68/100\n",
            "116/116 [==============================] - 49s 423ms/step - loss: 4.3057\n",
            "Epoch 69/100\n",
            "116/116 [==============================] - 48s 408ms/step - loss: 4.2688\n",
            "Epoch 70/100\n",
            "116/116 [==============================] - 50s 430ms/step - loss: 4.2243\n",
            "Epoch 71/100\n",
            "116/116 [==============================] - 49s 419ms/step - loss: 4.1827\n",
            "Epoch 72/100\n",
            "116/116 [==============================] - 49s 421ms/step - loss: 4.1429\n",
            "Epoch 73/100\n",
            "116/116 [==============================] - 47s 407ms/step - loss: 4.1043\n",
            "Epoch 74/100\n",
            "116/116 [==============================] - 47s 406ms/step - loss: 4.0641\n",
            "Epoch 75/100\n",
            "116/116 [==============================] - 48s 411ms/step - loss: 4.0253\n",
            "Epoch 76/100\n",
            "116/116 [==============================] - 49s 418ms/step - loss: 3.9939\n",
            "Epoch 77/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 3.9470\n",
            "Epoch 78/100\n",
            "116/116 [==============================] - 47s 407ms/step - loss: 3.9132\n",
            "Epoch 79/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 3.8748\n",
            "Epoch 80/100\n",
            "116/116 [==============================] - 48s 417ms/step - loss: 3.8337\n",
            "Epoch 81/100\n",
            "116/116 [==============================] - 48s 413ms/step - loss: 3.8049\n",
            "Epoch 82/100\n",
            "116/116 [==============================] - 47s 407ms/step - loss: 3.7649\n",
            "Epoch 83/100\n",
            "116/116 [==============================] - 47s 407ms/step - loss: 3.7269\n",
            "Epoch 84/100\n",
            "116/116 [==============================] - 48s 413ms/step - loss: 3.6955\n",
            "Epoch 85/100\n",
            "116/116 [==============================] - 49s 419ms/step - loss: 3.6519\n",
            "Epoch 86/100\n",
            "116/116 [==============================] - 47s 405ms/step - loss: 3.6159\n",
            "Epoch 87/100\n",
            "116/116 [==============================] - 47s 406ms/step - loss: 3.5801\n",
            "Epoch 88/100\n",
            "116/116 [==============================] - 47s 403ms/step - loss: 3.5581\n",
            "Epoch 89/100\n",
            "116/116 [==============================] - 48s 410ms/step - loss: 3.5179\n",
            "Epoch 90/100\n",
            "116/116 [==============================] - 49s 421ms/step - loss: 3.4839\n",
            "Epoch 91/100\n",
            "116/116 [==============================] - 47s 406ms/step - loss: 3.4480\n",
            "Epoch 92/100\n",
            "116/116 [==============================] - 47s 406ms/step - loss: 3.4268\n",
            "Epoch 93/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 3.3878\n",
            "Epoch 94/100\n",
            "116/116 [==============================] - 48s 419ms/step - loss: 3.3474\n",
            "Epoch 95/100\n",
            "116/116 [==============================] - 48s 411ms/step - loss: 3.3212\n",
            "Epoch 96/100\n",
            "116/116 [==============================] - 47s 408ms/step - loss: 3.2865\n",
            "Epoch 97/100\n",
            "116/116 [==============================] - 48s 411ms/step - loss: 3.2576\n",
            "Epoch 98/100\n",
            "116/116 [==============================] - 48s 412ms/step - loss: 3.2268\n",
            "Epoch 99/100\n",
            "116/116 [==============================] - 48s 417ms/step - loss: 3.1973\n",
            "Epoch 100/100\n",
            "116/116 [==============================] - 47s 405ms/step - loss: 3.1617\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ce557ee4220>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('modelfile.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx7HZnRyU0Ix",
        "outputId": "c3da4c07-19d4-4d0a-b00e-c99b31e99a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_model.keras')"
      ],
      "metadata": {
        "id": "URm1guhRnkb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, num_words, model, word_to_index, index_to_word):\n",
        "    text = []\n",
        "    for _ in range(num_words):\n",
        "        encoded = [word_to_index[word] for word in seed_text.split() if word in word_to_index]\n",
        "        encoded = pad_sequences([encoded], maxlen=sequence_length-1, padding='pre')\n",
        "        y_pred = model.predict(encoded, verbose=0)\n",
        "\n",
        "        # Get the index with the highest probability\n",
        "        predicted_index = np.argmax(y_pred, axis=-1)[0]\n",
        "        predicted_word = index_to_word[predicted_index]\n",
        "\n",
        "        seed_text += ' ' + predicted_word\n",
        "        text.append(predicted_word)\n",
        "    return ' '.join(text)\n"
      ],
      "metadata": {
        "id": "SybP6GG8DQKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Your generate_text function here (make sure to define necessary variables like model, word_to_index, index_to_word, sequence_length)\n",
        "\n",
        "# Streamlit interface\n",
        "st.title('Movie Script Generation App')\n",
        "\n",
        "# Text input for seed_text\n",
        "seed_text = st.text_input('Seed Text', 'Enter your seed text here')\n",
        "\n",
        "# Numeric input for num_words\n",
        "num_words = st.number_input('Number of Words to Generate', min_value=1, max_value=100, value=25)\n",
        "\n",
        "# Button to generate text\n",
        "if st.button('Generate Text'):\n",
        "    generated_text = generate_text(seed_text, num_words, model, word_to_index, index_to_word)\n",
        "    print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mikvQ75rDUqY",
        "outputId": "f3b4fe54-080d-45f9-90be-21153d142996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sinks convoy en Cramming noodle back pants weight door killin’ sheer violence Marcel sat sympathetic French beam emanating neck violence lion HAMMERSMARK puffing underneath man soldier entire black feet gold tableau certain Palu NAZI ENLISTED MEN moved napkin wall gonna one truck opposite end table Nazi knife marquee saying behind French dialogue going Basterds worse German Herrman dead Nazi caricatures ladies burning precede makes grab KNIFE rodent skin Monsieur LaPadite rumors heard regarding back us agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German classic films However anyway winning beyond lie hostess medical pop file cinema approaching gold Nancy pipe Shosanna’s cinema cry like week guards/ushers now-vacant cinema Goering proper neighbor They’ve marquee SPRAYS appears left resembles place indication war tonight ’bout table It’s agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German classic films However anyway winning beyond lie hostess medical pop file cinema approaching gold Nancy pipe Shosanna’s cinema cry like week guards/ushers now-vacant cinema Goering proper neighbor They’ve marquee SPRAYS appears left resembles place indication war tonight ’bout table It’s agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German classic films However anyway winning beyond lie hostess medical pop file cinema approaching gold Nancy pipe Shosanna’s cinema cry like week guards/ushers now-vacant cinema Goering proper neighbor They’ve marquee SPRAYS appears left resembles place indication war tonight ’bout table It’s agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German classic films However anyway winning beyond lie hostess medical pop file cinema approaching gold Nancy pipe Shosanna’s cinema cry like week guards/ushers now-vacant cinema Goering proper neighbor They’ve marquee SPRAYS appears left resembles place indication war tonight ’bout table It’s agonizing Louisaiane swill star false often Fredrick’s black fishnet veil face FLASK fighting one says S.S cap German found occur stinks playing kaput without posters kiosks one table interpreter S.S takes studies taking German Nazi taking occur short kaput calabash rat scamper door momentarily LOCKING folder now-vacant cinema entrance Inside cigarette FIRST German\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtM0smQjLW_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Personal Contribution Statement"
      ],
      "metadata": {
        "id": "ZF7Fz-TzfNPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####As I am working on a Solo project, all the tasks were contributed by me.The development of the text generation model from a movie script was intensive and strategically planned over a three-week period, encompassing various key stages of the project.\n",
        "\n",
        "####Week 1: Script Preparation and Feature Extraction\n",
        "\n",
        "Script Preprocessing: Initiated the project by cleaning, formatting, and structurally organizing the movie script for computational analysis.\n",
        "Feature Extraction: Implemented tokenization of the script, removing stop words and punctuation, and created a structured dataset. This included building a frequency-based vocabulary essential for the model training.\n",
        "####Week 2: Model Development and Initial Training\n",
        "\n",
        "Data Preparation: Prepared the data for the neural network by generating and formatting sequences from the tokenized script, including one-hot encoding of output labels.\n",
        "Model Design and Commencement of Training: Designed the LSTM-based neural network model, selecting the appropriate architecture and configuring layers. Initiated the model training, focusing on embedding, LSTM layers, dropout, and dense layers.\n",
        "####Week 3: Model Optimization and Text Generation\n",
        "\n",
        "Model Fine-Tuning: Continued with the training of the model, making iterative adjustments and refinements to optimize its performance and accuracy.\n",
        "Text Generation Implementation and Testing: Developed the text generation functionality to enable the model to produce new script content. Conducted thorough testing and evaluation of the generated text, ensuring coherence and alignment with the style of the input script."
      ],
      "metadata": {
        "id": "eBqoDkLnfUe-"
      }
    }
  ]
}